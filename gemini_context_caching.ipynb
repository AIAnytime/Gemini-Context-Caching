{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install -U google-generativeai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 880
        },
        "collapsed": true,
        "id": "l-txkUB5CAHO",
        "outputId": "ba5de438-bd23-4fed-e82a-a7758784e803"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: google-generativeai in /usr/local/lib/python3.10/dist-packages (0.5.4)\n",
            "Collecting google-generativeai\n",
            "  Downloading google_generativeai-0.7.0-py3-none-any.whl (163 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m163.1/163.1 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting google-ai-generativelanguage==0.6.5 (from google-generativeai)\n",
            "  Downloading google_ai_generativelanguage-0.6.5-py3-none-any.whl (717 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m717.3/717.3 kB\u001b[0m \u001b[31m21.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: google-api-core in /usr/local/lib/python3.10/dist-packages (from google-generativeai) (2.11.1)\n",
            "Requirement already satisfied: google-api-python-client in /usr/local/lib/python3.10/dist-packages (from google-generativeai) (2.84.0)\n",
            "Requirement already satisfied: google-auth>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from google-generativeai) (2.27.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from google-generativeai) (3.20.3)\n",
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.10/dist-packages (from google-generativeai) (2.7.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from google-generativeai) (4.66.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from google-generativeai) (4.12.2)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.10/dist-packages (from google-ai-generativelanguage==0.6.5->google-generativeai) (1.23.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth>=2.15.0->google-generativeai) (5.3.3)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth>=2.15.0->google-generativeai) (0.4.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth>=2.15.0->google-generativeai) (4.9)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core->google-generativeai) (1.63.1)\n",
            "Requirement already satisfied: requests<3.0.0.dev0,>=2.18.0 in /usr/local/lib/python3.10/dist-packages (from google-api-core->google-generativeai) (2.31.0)\n",
            "Requirement already satisfied: httplib2<1dev,>=0.15.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client->google-generativeai) (0.22.0)\n",
            "Requirement already satisfied: google-auth-httplib2>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client->google-generativeai) (0.1.1)\n",
            "Requirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client->google-generativeai) (4.1.1)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic->google-generativeai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.4 in /usr/local/lib/python3.10/dist-packages (from pydantic->google-generativeai) (2.18.4)\n",
            "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core->google-generativeai) (1.64.1)\n",
            "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core->google-generativeai) (1.48.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /usr/local/lib/python3.10/dist-packages (from httplib2<1dev,>=0.15.0->google-api-python-client->google-generativeai) (3.1.2)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=2.15.0->google-generativeai) (0.6.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core->google-generativeai) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core->google-generativeai) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core->google-generativeai) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core->google-generativeai) (2024.6.2)\n",
            "Installing collected packages: google-ai-generativelanguage, google-generativeai\n",
            "  Attempting uninstall: google-ai-generativelanguage\n",
            "    Found existing installation: google-ai-generativelanguage 0.6.4\n",
            "    Uninstalling google-ai-generativelanguage-0.6.4:\n",
            "      Successfully uninstalled google-ai-generativelanguage-0.6.4\n",
            "  Attempting uninstall: google-generativeai\n",
            "    Found existing installation: google-generativeai 0.5.4\n",
            "    Uninstalling google-generativeai-0.5.4:\n",
            "      Successfully uninstalled google-generativeai-0.5.4\n",
            "Successfully installed google-ai-generativelanguage-0.6.5 google-generativeai-0.7.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google"
                ]
              },
              "id": "c9a9bf1d6f054bf3a388d076b7affde3"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "aroyMVS15dVN"
      },
      "outputs": [],
      "source": [
        "import google.generativeai as genai\n",
        "import os\n",
        "from google.colab import userdata\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"GOOGLE_API_KEY\"] = userdata.get(\"GOOGLE_API_KEY\")\n",
        "genai.configure(api_key=os.environ['GOOGLE_API_KEY'])"
      ],
      "metadata": {
        "id": "IIalVMrMDjyX"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "video_file_name = \"sam.mp4\""
      ],
      "metadata": {
        "id": "xZPkEot4B-mm"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "video_file = genai.upload_file(path=video_file_name)"
      ],
      "metadata": {
        "id": "V_u5ooV7C1zJ"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "while video_file.state.name == \"PROCESSING\":\n",
        "    print('Waiting for video to be processed.')\n",
        "    time.sleep(2)\n",
        "    video_file = genai.get_file(video_file.name)\n",
        "\n",
        "print(f'Video processing complete: ' + video_file.uri)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "f5QfxAG7DIAL",
        "outputId": "de6d27de-0154-4d19-d83e-4e6af23443e2"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Waiting for video to be processed.\n",
            "Video processing complete: https://generativelanguage.googleapis.com/v1beta/files/p68jd6kcni96\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\" Context caching \" is a feature that allows you to reduce cost and latency by caching input tokens and referencing the cached tokens in subsequent requests.\n",
        "\n",
        "When you store a token in the cache, you specify the cache duration ( TTL ) for the token. The cost of storing the cache depends on the size of the input token and the duration for which the token is kept.\n",
        "\n",
        "\"Context caching\" is supported in both \"Gemini 1.5 Pro\" and \"Gemini 1.5 Flash\"."
      ],
      "metadata": {
        "id": "abPG_fHqQ0sd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.generativeai import caching\n",
        "import datetime"
      ],
      "metadata": {
        "id": "U2Z4mZHVEIDf"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use cases for Context caching\n",
        "Context caching is appropriate when the initial context is referenced repeatedly by short requests.\n",
        "\n",
        "Chatbots with long system instructions Repetitive analysis of long video files Periodic queries against large document sets Frequent code repository analysis Bug fixing\n"
      ],
      "metadata": {
        "id": "_NAiGivVQwGs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cache = caching.CachedContent.create(\n",
        "    model=\"models/gemini-1.5-flash-001\",\n",
        "    display_name=\"AI Anytime Video\",\n",
        "    system_instruction=\"You are an expert video analyzer, and your task is to answer user's query based on the video file you have access to.\",\n",
        "    contents=[video_file],\n",
        "    ttl=datetime.timedelta(minutes=5),\n",
        ")"
      ],
      "metadata": {
        "id": "YEa-ELWPDH9b"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = genai.GenerativeModel.from_cached_content(cached_content=cache)"
      ],
      "metadata": {
        "id": "wl_E3s2cDH6f"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = model.generate_content(\n",
        "    [\"What is this video all about?\"]\n",
        ")"
      ],
      "metadata": {
        "id": "Z6I6PDDTPk4L"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0sSAFlJ5Pt0G",
        "outputId": "55349afd-285b-4e57-a7e9-b230fc06317f"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "response:\n",
              "GenerateContentResponse(\n",
              "    done=True,\n",
              "    iterator=None,\n",
              "    result=protos.GenerateContentResponse({\n",
              "      \"candidates\": [\n",
              "        {\n",
              "          \"content\": {\n",
              "            \"parts\": [\n",
              "              {\n",
              "                \"text\": \"This video is an interview with Sam Altman, CEO of OpenAI, about his advice on work-life balance in your twenties. He stresses that working hard early in your career is important for long-term success.  He also talks about the importance of enjoying your work and believing in what you're doing to stay motivated and push through the difficult times. \\n\"\n",
              "              }\n",
              "            ],\n",
              "            \"role\": \"model\"\n",
              "          },\n",
              "          \"finish_reason\": \"STOP\",\n",
              "          \"index\": 0,\n",
              "          \"safety_ratings\": [\n",
              "            {\n",
              "              \"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\",\n",
              "              \"probability\": \"NEGLIGIBLE\"\n",
              "            },\n",
              "            {\n",
              "              \"category\": \"HARM_CATEGORY_HATE_SPEECH\",\n",
              "              \"probability\": \"NEGLIGIBLE\"\n",
              "            },\n",
              "            {\n",
              "              \"category\": \"HARM_CATEGORY_HARASSMENT\",\n",
              "              \"probability\": \"NEGLIGIBLE\"\n",
              "            },\n",
              "            {\n",
              "              \"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\",\n",
              "              \"probability\": \"NEGLIGIBLE\"\n",
              "            }\n",
              "          ]\n",
              "        }\n",
              "      ],\n",
              "      \"usage_metadata\": {\n",
              "        \"prompt_token_count\": 77326,\n",
              "        \"candidates_token_count\": 72,\n",
              "        \"total_token_count\": 77398,\n",
              "        \"cached_content_token_count\": 77318\n",
              "      }\n",
              "    }),\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(response.usage_metadata)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KGWD-UxEPz7K",
        "outputId": "4d537f9c-fc0b-4ffe-a6b0-aa7ec5a02435"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prompt_token_count: 77326\n",
            "candidates_token_count: 72\n",
            "total_token_count: 77398\n",
            "cached_content_token_count: 77318\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "・The minimum number of input tokens for Context caching is 32,768, and the maximum is the same as the maximum for the specified model.\n",
        "\n",
        "・If the cache retention time (TTL) is not set, it is 1 hour.\n",
        "\n",
        "・The model does not distinguish between cached and normal tokens. Cached content is simply prefixed to the prompt.\n",
        "\n",
        "・The cache service provides a delete operation to manually delete content from the cache.\n",
        "\n",
        "・In paid versions, there are no special rate or usage limits for Context caching. Standard rate limits apply to GenerateContent, and the token limit includes cached tokens. In free versions, \"Gemini 1.5 Flash\" has a storage limit of 1 million tokens, and caching is not available in \"Gemini 1.5 Pro\".\n",
        "\n",
        "・You cannot retrieve or display cached content, but you can retrieve the metadata (name, display_name, model, and create, update, expire times).\n",
        "\n",
        "・You can update the display_name and set a new ttl or expire_time. No other changes are supported.\n",
        "\n",
        ". The number of cached tokens is returned by the create, get, and list operations of the usage_metadata cache service, and also when using GenerateContent caching."
      ],
      "metadata": {
        "id": "q7IA6CvTQMbJ"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wljTWW8lQEgK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}